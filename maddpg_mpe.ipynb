{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"maddpg_mpe.ipynb","version":"0.3.2","provenance":[{"file_id":"1Q21lHD2ldgO91vyg9EzeHEv-H8DV7vAU","timestamp":1543051917557},{"file_id":"https://github.com/udacity/deep-reinforcement-learning/blob/master/dqn/solution/Deep_Q_Network_Solution.ipynb","timestamp":1540716933998}],"collapsed_sections":["bfUaPec7GP_m"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"2NeuThdPVknh","colab_type":"text"},"cell_type":"markdown","source":["# Tensorflow Implementation of MADDPG in Multiagent-Particle Environments\n","\n","---\n","\n","### 1. Install Gym\n"]},{"metadata":{"id":"fj4ioW2PVnbC","colab_type":"code","outputId":"b14548ee-cb68-4c41-d7a3-92e301f806c1","executionInfo":{"status":"ok","timestamp":1543134805256,"user_tz":-480,"elapsed":77204,"user":{"displayName":"Saul Leung","photoUrl":"https://lh4.googleusercontent.com/--atDq9mMZEc/AAAAAAAAAAI/AAAAAAAAcVs/twfLrf9myYA/s64/photo.jpg","userId":"06393280560991383795"}},"colab":{"base_uri":"https://localhost:8080/","height":904}},"cell_type":"code","source":["# Install gym\n","!apt-get update\n","!apt-get install xvfb ffmpeg python-opengl > /dev/null\n","!pip3 install gym\n","!pip3 install pyvirtualdisplay"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64  InRelease\n","Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [83.2 kB]\n","Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  InRelease\n","Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64  Release\n","Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release\n","Hit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [264 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [734 kB]\n","Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [119 kB]\n","Get:15 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [1,364 B]\n","Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [574 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [6,138 B]\n","Fetched 1,944 kB in 3s (610 kB/s)\n","Reading package lists... Done\n","Extracting templates from packages: 100%\n","Collecting gym\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/22/4ff09745ade385ffe707fb5f053548f0f6a6e7d5e98a2b9d6c07f5b931a7/gym-0.10.9.tar.gz (1.5MB)\n","\u001b[K    100% |████████████████████████████████| 1.5MB 16.7MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n","Collecting pyglet>=1.2.0 (from gym)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n","\u001b[K    100% |████████████████████████████████| 1.0MB 19.5MB/s \n","\u001b[?25hRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.10.15)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n","Building wheels for collected packages: gym\n","  Running setup.py bdist_wheel for gym ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/6c/3a/0e/b86dee98876bb56cdb482cc1f72201035e46d1baf69d10d028\n","Successfully built gym\n","Installing collected packages: pyglet, gym\n","Successfully installed gym-0.10.9 pyglet-1.3.2\n","Collecting pyvirtualdisplay\n","  Downloading https://files.pythonhosted.org/packages/39/37/f285403a09cc261c56b6574baace1bdcf4b8c7428c8a7239cbba137bc0eb/PyVirtualDisplay-0.2.1.tar.gz\n","Collecting EasyProcess (from pyvirtualdisplay)\n","  Downloading https://files.pythonhosted.org/packages/0d/f1/d2de7591e7dfc164d286fa16f051e6c0cf3141825586c3b04ae7cda7ac0f/EasyProcess-0.2.3.tar.gz\n","Building wheels for collected packages: pyvirtualdisplay, EasyProcess\n","  Running setup.py bdist_wheel for pyvirtualdisplay ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d1/8c/16/1c64227974ae29c687e4cc30fd691d5c0fd40f54446dde99da\n","  Running setup.py bdist_wheel for EasyProcess ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b4/c6/e3/c163b04029d8fccfd54b809802640c1af587a01be8d7a04e1a\n","Successfully built pyvirtualdisplay EasyProcess\n","Installing collected packages: EasyProcess, pyvirtualdisplay\n","Successfully installed EasyProcess-0.2.3 pyvirtualdisplay-0.2.1\n"],"name":"stdout"}]},{"metadata":{"id":"X8JxfuSnVAPO","colab_type":"text"},"cell_type":"markdown","source":["### 2. Import the Necessary Packages"]},{"metadata":{"id":"CVpXI-d5Vknk","colab_type":"code","colab":{}},"cell_type":"code","source":["import gym\n","import numpy as np\n","import random\n","import tensorflow as tf\n","import time\n","import pickle\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Activate virtual display\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"28rRmasFDvpN","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.animation\n","from IPython import display as ipythondisplay\n","from IPython.display import HTML\n","\n","def display_frames_as_gif(frames):\n","    \"\"\"\n","    Displays a list of frames as a gif, with controls\n","    \"\"\"\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","    animate = lambda i: patch.set_data(frames[i])\n","    ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n","    HTML(ani.to_jshtml())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bfUaPec7GP_m","colab_type":"text"},"cell_type":"markdown","source":["### 3. Install the Multiagent-Particle Environment (Optional)"]},{"metadata":{"id":"PvOgoL7wHh5Z","colab_type":"code","outputId":"e1fbc1d1-a60c-4fd5-e133-15dee877e82a","executionInfo":{"status":"ok","timestamp":1543134919910,"user_tz":-480,"elapsed":31971,"user":{"displayName":"Saul Leung","photoUrl":"https://lh4.googleusercontent.com/--atDq9mMZEc/AAAAAAAAAAI/AAAAAAAAcVs/twfLrf9myYA/s64/photo.jpg","userId":"06393280560991383795"}},"colab":{"base_uri":"https://localhost:8080/","height":360}},"cell_type":"code","source":["# Mounting Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change working directory\n","import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks/move37-final/')\n","!ls -l"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","total 118\n","drwx------ 2 root root  4096 Nov 24 10:05 bin\n","-rw------- 1 root root 11853 Nov 24 15:31 distributions.py\n","-rw------- 1 root root  1063 Nov 24 10:05 LICENSE.txt\n","-rw------- 1 root root 61602 Nov 25 08:34 maddpg_multiagent-particle.ipynb\n","-rw------- 1 root root  1916 Nov 24 10:05 make_env.py\n","drwx------ 2 root root  4096 Nov 25 06:22 models\n","drwx------ 2 root root  4096 Nov 24 10:05 multiagent\n","drwx------ 2 root root  4096 Nov 24 10:05 multiagent.egg-info\n","drwx------ 2 root root  4096 Nov 24 13:18 __pycache__\n","-rw------- 1 root root  6443 Nov 24 10:05 README.md\n","-rw------- 1 root root  2788 Nov 24 12:14 replay_buffer.py\n","-rw------- 1 root root   426 Nov 24 10:05 setup.py\n","-rw------- 1 root root 11887 Nov 24 15:29 utils.py\n"],"name":"stdout"}]},{"metadata":{"id":"dKYyjJaCGJ84","colab_type":"code","outputId":"f0d9372a-2264-419d-f328-f49da206849a","executionInfo":{"status":"ok","timestamp":1543053935623,"user_tz":-480,"elapsed":9395,"user":{"displayName":"Saul Leung","photoUrl":"https://lh4.googleusercontent.com/--atDq9mMZEc/AAAAAAAAAAI/AAAAAAAAcVs/twfLrf9myYA/s64/photo.jpg","userId":"06393280560991383795"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"cell_type":"code","source":["# Git clone into the current directory\n","!git init .\n","!git remote add -t \\* -f origin https://github.com/openai/multiagent-particle-envs.git\n","!git checkout master"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized empty Git repository in /content/drive/My Drive/Colab Notebooks/move37-final/.git/\n","Updating origin\n","remote: Enumerating objects: 227, done.\u001b[K\n","remote: Total 227 (delta 0), reused 0 (delta 0), pack-reused 227\u001b[K\n","Receiving objects: 100% (227/227), 99.67 KiB | 289.00 KiB/s, done.\n","Resolving deltas: 100% (123/123), done.\n","From https://github.com/openai/multiagent-particle-envs\n"," * [new branch]      master     -> origin/master\n","Branch 'master' set up to track remote branch 'master' from 'origin'.\n","Already on 'master'\n"],"name":"stdout"}]},{"metadata":{"id":"2x_ZHTkSIcRP","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install -e ."],"execution_count":0,"outputs":[]},{"metadata":{"id":"VvnVl_NJVknn","colab_type":"text"},"cell_type":"markdown","source":["### 4. Instantiate the Environment and Agent"]},{"metadata":{"id":"rFWXhrMT2G0L","colab_type":"code","outputId":"eef58594-5539-45c7-9a7a-5a1c83d76a3b","executionInfo":{"status":"ok","timestamp":1543134926971,"user_tz":-480,"elapsed":4107,"user":{"displayName":"Saul Leung","photoUrl":"https://lh4.googleusercontent.com/--atDq9mMZEc/AAAAAAAAAAI/AAAAAAAAcVs/twfLrf9myYA/s64/photo.jpg","userId":"06393280560991383795"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Allow relative imports to directories above the current directory\n","import os, sys\n","sys.path.append('.')\n","\n","#import modules\n","from importlib import reload\n","from replay_buffer import ReplayBuffer\n","from distributions import make_pdtype\n","import utils as U; reload(U)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<module 'utils' from '/content/drive/My Drive/Colab Notebooks/move37-final/utils.py'>"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"hK7wwP-arj1F","colab_type":"code","colab":{}},"cell_type":"code","source":["# Training parameters\n","MAX_EPISODE_LEN = int(25)  # maximum episode length\n","NUM_EPISODES = int(25000)  # number of episodes\n","NUM_ADVERSARIES = int(0)  # number of adversaries\n","LR = 1e-2  # learning rate for Adam optimizer\n","GAMMA = 0.95  # discount factor\n","BATCH_SIZE = int(1024)  # number of episodes to optimize at the same time\n","NUM_UNITS = int(64)  # number of units in the mlp\n","GOOD_POLICY = 'maddpg'  # policy of good agents\n","ADV_POLICY = 'maddpg'  # policy of adversaries\n","# Checkpointing\n","SAVE_DIR = './models/'  # directory in which training state and model should be saved\n","SAVE_RATE = int(1000)  # save model once every time this many episodes are completed\n","LOAD_DIR = ''  # directory in which training state and model are loaded\n","# Evaluation\n","RESTORE = False\n","SAVE_GIFS = False\n","BENCHMARK = False\n","BENCHMARK_ITERS = int(100000)  # number of iterations run for benchmarking\n","BENCHMARK_DIR = './benchmark_files/'  # directory where benchmark data is saved\n","GIFS_DIR = './gifs/'  # directory where image data is saved\n","\n","def discount_with_dones(rewards, dones, gamma):\n","    discounted = []\n","    r = 0\n","    for reward, done in zip(rewards[::-1], dones[::-1]):\n","        r = reward + gamma*r\n","        r = r*(1.-done)\n","        discounted.append(r)\n","    return discounted[::-1]\n","\n","def make_update_exp(vals, target_vals):\n","    polyak = 1.0 - 1e-2\n","    expression = []\n","    for var, var_target in zip(sorted(vals, key=lambda v: v.name), sorted(target_vals, key=lambda v: v.name)):\n","        expression.append(var_target.assign(polyak * var_target + (1.0-polyak) * var))\n","    expression = tf.group(*expression)\n","    return U.function([], [], updates=[expression])\n","\n","def p_train(make_obs_ph_n, act_space_n, p_index, p_func, q_func, optimizer, grad_norm_clipping=None, local_q_func=False, num_units=64, scope=\"trainer\", reuse=None):\n","    with tf.variable_scope(scope, reuse=reuse):\n","        # create distribtuions\n","        act_pdtype_n = [make_pdtype(act_space) for act_space in act_space_n]\n","\n","        # set up placeholders\n","        obs_ph_n = make_obs_ph_n\n","        act_ph_n = [act_pdtype_n[i].sample_placeholder([None], name=\"action\"+str(i)) for i in range(len(act_space_n))]\n","\n","        p_input = obs_ph_n[p_index]\n","\n","        p = p_func(p_input, int(act_pdtype_n[p_index].param_shape()[0]), scope=\"p_func\", num_units=num_units)\n","        p_func_vars = U.scope_vars(U.absolute_scope_name(\"p_func\"))\n","\n","        # wrap parameters in distribution\n","        act_pd = act_pdtype_n[p_index].pdfromflat(p)\n","\n","        act_sample = act_pd.sample()\n","        p_reg = tf.reduce_mean(tf.square(act_pd.flatparam()))\n","\n","        act_input_n = act_ph_n + []\n","        act_input_n[p_index] = act_pd.sample()\n","        q_input = tf.concat(obs_ph_n + act_input_n, 1)\n","        if local_q_func:\n","            q_input = tf.concat([obs_ph_n[p_index], act_input_n[p_index]], 1)\n","        q = q_func(q_input, 1, scope=\"q_func\", reuse=True, num_units=num_units)[:,0]\n","        pg_loss = -tf.reduce_mean(q)\n","\n","        loss = pg_loss + p_reg * 1e-3\n","\n","        optimize_expr = U.minimize_and_clip(optimizer, loss, p_func_vars, grad_norm_clipping)\n","\n","        # Create callable functions\n","        train = U.function(inputs=obs_ph_n + act_ph_n, outputs=loss, updates=[optimize_expr])\n","        act = U.function(inputs=[obs_ph_n[p_index]], outputs=act_sample)\n","        p_values = U.function([obs_ph_n[p_index]], p)\n","\n","        # target network\n","        target_p = p_func(p_input, int(act_pdtype_n[p_index].param_shape()[0]), scope=\"target_p_func\", num_units=num_units)\n","        target_p_func_vars = U.scope_vars(U.absolute_scope_name(\"target_p_func\"))\n","        update_target_p = make_update_exp(p_func_vars, target_p_func_vars)\n","\n","        target_act_sample = act_pdtype_n[p_index].pdfromflat(target_p).sample()\n","        target_act = U.function(inputs=[obs_ph_n[p_index]], outputs=target_act_sample)\n","\n","        return act, train, update_target_p, {'p_values': p_values, 'target_act': target_act}\n","\n","def q_train(make_obs_ph_n, act_space_n, q_index, q_func, optimizer, grad_norm_clipping=None, local_q_func=False, scope=\"trainer\", reuse=None, num_units=64):\n","    with tf.variable_scope(scope, reuse=reuse):\n","        # create distribtuions\n","        act_pdtype_n = [make_pdtype(act_space) for act_space in act_space_n]\n","\n","        # set up placeholders\n","        obs_ph_n = make_obs_ph_n\n","        act_ph_n = [act_pdtype_n[i].sample_placeholder([None], name=\"action\"+str(i)) for i in range(len(act_space_n))]\n","        target_ph = tf.placeholder(tf.float32, [None], name=\"target\")\n","\n","        q_input = tf.concat(obs_ph_n + act_ph_n, 1)\n","        if local_q_func:\n","            q_input = tf.concat([obs_ph_n[q_index], act_ph_n[q_index]], 1)\n","        q = q_func(q_input, 1, scope=\"q_func\", num_units=num_units)[:,0]\n","        q_func_vars = U.scope_vars(U.absolute_scope_name(\"q_func\"))\n","\n","        q_loss = tf.reduce_mean(tf.square(q - target_ph))\n","\n","        # viscosity solution to Bellman differential equation in place of an initial condition\n","        q_reg = tf.reduce_mean(tf.square(q))\n","        loss = q_loss #+ 1e-3 * q_reg\n","\n","        optimize_expr = U.minimize_and_clip(optimizer, loss, q_func_vars, grad_norm_clipping)\n","\n","        # Create callable functions\n","        train = U.function(inputs=obs_ph_n + act_ph_n + [target_ph], outputs=loss, updates=[optimize_expr])\n","        q_values = U.function(obs_ph_n + act_ph_n, q)\n","\n","        # target network\n","        target_q = q_func(q_input, 1, scope=\"target_q_func\", num_units=num_units)[:,0]\n","        target_q_func_vars = U.scope_vars(U.absolute_scope_name(\"target_q_func\"))\n","        update_target_q = make_update_exp(q_func_vars, target_q_func_vars)\n","\n","        target_q_values = U.function(obs_ph_n + act_ph_n, target_q)\n","\n","        return train, update_target_q, {'q_values': q_values, 'target_q_values': target_q_values}\n","\n","class AgentTrainer(object):\n","    def __init__(self, name, model, obs_shape, act_space):\n","        raise NotImplemented()\n","\n","    def action(self, obs):\n","        raise NotImplemented()\n","\n","    def process_experience(self, obs, act, rew, new_obs, done, terminal):\n","        raise NotImplemented()\n","\n","    def preupdate(self):\n","        raise NotImplemented()\n","\n","    def update(self, agents):\n","        raise NotImplemented()\n","      \n","class MADDPGAgentTrainer(AgentTrainer):\n","    def __init__(self, name, model, obs_shape_n, act_space_n, agent_index, local_q_func=False):\n","        self.name = name\n","        self.n = len(obs_shape_n)\n","        self.agent_index = agent_index\n","        obs_ph_n = []\n","        for i in range(self.n):\n","            obs_ph_n.append(U.BatchInput(obs_shape_n[i], name=\"observation\"+str(i)).get())\n","\n","        # Create all the functions necessary to train the model\n","        self.q_train, self.q_update, self.q_debug = q_train(\n","            scope=self.name,\n","            make_obs_ph_n=obs_ph_n,\n","            act_space_n=act_space_n,\n","            q_index=agent_index,\n","            q_func=model,\n","            optimizer=tf.train.AdamOptimizer(learning_rate=LR),\n","            grad_norm_clipping=0.5,\n","            local_q_func=local_q_func,\n","            num_units=NUM_UNITS\n","        )\n","        self.act, self.p_train, self.p_update, self.p_debug = p_train(\n","            scope=self.name,\n","            make_obs_ph_n=obs_ph_n,\n","            act_space_n=act_space_n,\n","            p_index=agent_index,\n","            p_func=model,\n","            q_func=model,\n","            optimizer=tf.train.AdamOptimizer(learning_rate=LR),\n","            grad_norm_clipping=0.5,\n","            local_q_func=local_q_func,\n","            num_units=NUM_UNITS\n","        )\n","        # Create experience buffer\n","        self.replay_buffer = ReplayBuffer(1e6)\n","        self.max_replay_buffer_len = BATCH_SIZE * MAX_EPISODE_LEN\n","        self.replay_sample_index = None\n","\n","    def action(self, obs):\n","        return self.act(obs[None])[0]\n","\n","    def experience(self, obs, act, rew, new_obs, done, terminal):\n","        # Store transition in the replay buffer.\n","        self.replay_buffer.add(obs, act, rew, new_obs, float(done))\n","\n","    def preupdate(self):\n","        self.replay_sample_index = None\n","\n","    def update(self, agents, t):\n","        if len(self.replay_buffer) < self.max_replay_buffer_len: # replay buffer is not large enough\n","            return\n","        if not t % 100 == 0:  # only update every 100 steps\n","            return\n","\n","        self.replay_sample_index = self.replay_buffer.make_index(BATCH_SIZE)\n","        # collect replay sample from all agents\n","        obs_n = []\n","        obs_next_n = []\n","        act_n = []\n","        index = self.replay_sample_index\n","        for i in range(self.n):\n","            obs, act, rew, obs_next, done = agents[i].replay_buffer.sample_index(index)\n","            obs_n.append(obs)\n","            obs_next_n.append(obs_next)\n","            act_n.append(act)\n","        obs, act, rew, obs_next, done = self.replay_buffer.sample_index(index)\n","\n","        # train q network\n","        num_sample = 1\n","        target_q = 0.0\n","        for i in range(num_sample):\n","            target_act_next_n = [agents[i].p_debug['target_act'](obs_next_n[i]) for i in range(self.n)]\n","            target_q_next = self.q_debug['target_q_values'](*(obs_next_n + target_act_next_n))\n","            target_q += rew + GAMMA * (1.0 - done) * target_q_next\n","        target_q /= num_sample\n","        q_loss = self.q_train(*(obs_n + act_n + [target_q]))\n","\n","        # train p network\n","        p_loss = self.p_train(*(obs_n + act_n))\n","\n","        self.p_update()\n","        self.q_update()\n","\n","        return [q_loss, p_loss, np.mean(target_q), np.mean(rew), np.mean(target_q_next), np.std(target_q)]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"W3JYRlp4Vknp","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow.contrib.layers as layers\n","\n","def mlp_model(input, num_outputs, scope, reuse=False, num_units=64, rnn_cell=None):\n","    # This model takes as input an observation and returns values of all actions\n","    with tf.variable_scope(scope, reuse=reuse):\n","        out = input\n","        out = layers.fully_connected(out, num_outputs=num_units, activation_fn=tf.nn.relu)\n","        out = layers.fully_connected(out, num_outputs=num_units, activation_fn=tf.nn.relu)\n","        out = layers.fully_connected(out, num_outputs=num_outputs, activation_fn=None)\n","        return out\n","\n","def get_trainers(env, num_adversaries, obs_shape_n):\n","    trainers = []\n","    model = mlp_model\n","    trainer = MADDPGAgentTrainer\n","    for i in range(num_adversaries):\n","        trainers.append(trainer(\"agent_%d\" % i, model, obs_shape_n, env.action_space, i, local_q_func=(ADV_POLICY=='ddpg')))\n","    for i in range(num_adversaries, env.n):\n","        trainers.append(trainer(\"agent_%d\" % i, model, obs_shape_n, env.action_space, i, local_q_func=(GOOD_POLICY=='ddpg')))\n","    return trainers\n","\n","def train(scenario='simple', name='training'):\n","    from make_env import make_env\n"," \n","    tf.reset_default_graph()\n","\n","    with U.single_threaded_session():\n","        # Create environment\n","        env = make_env(scenario, BENCHMARK)\n","        # Create agent trainers\n","        obs_shape_n = [env.observation_space[i].shape for i in range(env.n)]\n","        num_adversaries = min(env.n, NUM_ADVERSARIES)\n","        trainers = get_trainers(env, num_adversaries, obs_shape_n)\n","        print('Using good policy {} and adv policy {}'.format(GOOD_POLICY, ADV_POLICY))\n","\n","        # Initialize\n","        U.initialize()\n","\n","        # Load previous results, if necessary\n","        load_dir = LOAD_DIR\n","        if load_dir == \"\":\n","            load_dir = SAVE_DIR\n","#         if DISPLAY or RESTORE or BENCHMARK:\n","        if RESTORE:\n","            print('Loading previous state...')\n","            U.load_state(load_dir + name)\n","\n","        episode_rewards = [0.0]  # sum of rewards for all agents\n","        agent_rewards = [[0.0] for _ in range(env.n)]  # individual agent reward\n","        final_ep_rewards = []  # sum of rewards for training curve\n","        final_ep_ag_rewards = []  # agent rewards for training curve\n","        agent_info = [[[]]]  # placeholder for benchmarking info\n","        saver = tf.train.Saver()\n","        obs_n = env.reset()\n","        episode_step = 0\n","        train_step = 0\n","        t_start = time.time()\n","        \n","        if SAVE_GIFS:\n","            frames = []\n","            frames.append(env.render(mode = 'rgb_array')[0])\n","             \n","        print('Starting iterations...')\n","        while True:\n","            # get action\n","            action_n = [agent.action(obs) for agent, obs in zip(trainers,obs_n)]\n","            # environment step\n","            new_obs_n, rew_n, done_n, info_n = env.step(action_n)\n","            episode_step += 1\n","            done = all(done_n)\n","            terminal = (episode_step >= MAX_EPISODE_LEN)\n","            # collect experience\n","            for i, agent in enumerate(trainers):\n","                agent.experience(obs_n[i], action_n[i], rew_n[i], new_obs_n[i], done_n[i], terminal)\n","            obs_n = new_obs_n\n","\n","            for i, rew in enumerate(rew_n):\n","                episode_rewards[-1] += rew\n","                agent_rewards[i][-1] += rew\n","\n","            if done or terminal:\n","                obs_n = env.reset()\n","                episode_step = 0\n","                episode_rewards.append(0)\n","                for a in agent_rewards:\n","                    a.append(0)\n","                agent_info.append([[]])\n","\n","            # increment global step counter\n","            train_step += 1\n","\n","            # for benchmarking learned policies\n","            if BENCHMARK:\n","                for i, info in enumerate(info_n):\n","                    agent_info[-1][i].append(info_n['n'])\n","                if train_step > BENCHMARK_ITES and (done or terminal):\n","                    file_name = BENCHMARK_DIR + name + '.pkl'\n","                    print('Finished benchmarking, now saving...')\n","                    with open(file_name, 'wb') as fp:\n","                        pickle.dump(agent_info[:-1], fp)\n","                    break\n","                continue\n","            \n","            # save frames for evaluation\n","            if SAVE_GIFS:\n","                time.sleep(0.1)\n","                frames.append(env.render(mode = 'rgb_array')[0])\n","                continue\n","                \n","            # update all trainers, if not in benchmark mode\n","            loss = None\n","            for agent in trainers:\n","                agent.preupdate()\n","            for agent in trainers:\n","                loss = agent.update(trainers, train_step)\n","\n","            # save model, display training output\n","            if terminal and (len(episode_rewards) % SAVE_RATE == 0):\n","                U.save_state(SAVE_DIR + name, saver=saver)\n","                # print statement depends on whether or not there are adversaries\n","                if num_adversaries == 0:\n","                    print(\"steps: {}, episodes: {}, mean episode reward: {}, time: {}\".format(\n","                        train_step, len(episode_rewards), np.mean(episode_rewards[-SAVE_RATE:]), round(time.time()-t_start, 3)))\n","                else:\n","                    print(\"steps: {}, episodes: {}, mean episode reward: {}, agent episode reward: {}, time: {}\".format(\n","                        train_step, len(episode_rewards), np.mean(episode_rewards[-SAVE_RATE:]),\n","                        [np.mean(rew[-SAVE_RATE:]) for rew in agent_rewards], round(time.time()-t_start, 3)))\n","                t_start = time.time()\n","                # Keep track of final episode reward\n","                final_ep_rewards.append(np.mean(episode_rewards[-SAVE_RATE:]))\n","                for rew in agent_rewards:\n","                    final_ep_ag_rewards.append(np.mean(rew[-SAVE_RATE:]))\n","            \n","            # finish the training process\n","            if len(episode_rewards) > NUM_EPISODES:\n","                print('...Finished total of {} episodes.'.format(len(episode_rewards)))\n","                break\n","\n","        env.close()\n","#         display_frames_as_gif(frames)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wuIBiUb_Y_zI","colab_type":"text"},"cell_type":"markdown","source":["### 5. Cooperative Communication\n","One agent is the ‘speaker’ (gray) that does not move (observes goal of other agent), and other agent is the listener (cannot speak, but must navigate to correct landmark)."]},{"metadata":{"id":"bfrBDETKZm9g","colab_type":"code","outputId":"2c03b3f1-0df1-4191-deaa-8213aadb902d","executionInfo":{"status":"ok","timestamp":1543136643279,"user_tz":-480,"elapsed":1660238,"user":{"displayName":"Saul Leung","photoUrl":"https://lh4.googleusercontent.com/--atDq9mMZEc/AAAAAAAAAAI/AAAAAAAAcVs/twfLrf9myYA/s64/photo.jpg","userId":"06393280560991383795"}},"colab":{"base_uri":"https://localhost:8080/","height":493}},"cell_type":"code","source":["train('simple_speaker_listener', 'Cooperative Communication')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Using good policy maddpg and adv policy maddpg\n","Starting iterations...\n","steps: 24975, episodes: 1000, mean episode reward: -142.88330606485846, time: 55.184\n","steps: 49975, episodes: 2000, mean episode reward: -119.55574798871612, time: 66.547\n","steps: 74975, episodes: 3000, mean episode reward: -58.635686984176616, time: 67.272\n","steps: 99975, episodes: 4000, mean episode reward: -57.471882724832874, time: 66.496\n","steps: 124975, episodes: 5000, mean episode reward: -60.433609603389606, time: 66.012\n","steps: 149975, episodes: 6000, mean episode reward: -59.22826531946425, time: 67.83\n","steps: 174975, episodes: 7000, mean episode reward: -58.554753915049986, time: 66.907\n","steps: 199975, episodes: 8000, mean episode reward: -59.84911301995979, time: 66.648\n","steps: 224975, episodes: 9000, mean episode reward: -56.50665237094993, time: 66.48\n","steps: 249975, episodes: 10000, mean episode reward: -58.40336454298203, time: 67.581\n","steps: 274975, episodes: 11000, mean episode reward: -60.89297352407632, time: 66.47\n","steps: 299975, episodes: 12000, mean episode reward: -61.558758542241975, time: 66.628\n","steps: 324975, episodes: 13000, mean episode reward: -61.857060080078845, time: 66.527\n","steps: 349975, episodes: 14000, mean episode reward: -61.57942751015175, time: 66.026\n","steps: 374975, episodes: 15000, mean episode reward: -59.73680094627361, time: 67.556\n","steps: 399975, episodes: 16000, mean episode reward: -61.03768083080712, time: 65.647\n","steps: 424975, episodes: 17000, mean episode reward: -57.93411736194292, time: 65.581\n","steps: 449975, episodes: 18000, mean episode reward: -57.47062062616925, time: 65.614\n","steps: 474975, episodes: 19000, mean episode reward: -57.854255563771886, time: 65.504\n","steps: 499975, episodes: 20000, mean episode reward: -59.40205878751163, time: 66.4\n","steps: 524975, episodes: 21000, mean episode reward: -60.44819775966559, time: 65.144\n","steps: 549975, episodes: 22000, mean episode reward: -59.641655479045035, time: 65.517\n","steps: 574975, episodes: 23000, mean episode reward: -60.89595782640566, time: 65.473\n","steps: 599975, episodes: 24000, mean episode reward: -57.5665699340908, time: 67.359\n","steps: 624975, episodes: 25000, mean episode reward: -57.53418053135794, time: 66.28\n","...Finished total of 25001 episodes.\n"],"name":"stdout"}]},{"metadata":{"id":"_vLRUEyRVknz","colab_type":"text"},"cell_type":"markdown","source":["### 6. Predator-Prey\n","Good agents (green) are faster and want to avoid being hit by adversaries (red). Adversaries are slower and want to hit good agents. Obstacles (large black circles) block the way.\n","\n"]},{"metadata":{"id":"GAQyX_y1Vknv","colab_type":"code","outputId":"47b27410-ede2-4b90-cf57-e93a7856148c","executionInfo":{"status":"ok","timestamp":1543140657969,"user_tz":-480,"elapsed":3837723,"user":{"displayName":"Saul Leung","photoUrl":"https://lh4.googleusercontent.com/--atDq9mMZEc/AAAAAAAAAAI/AAAAAAAAcVs/twfLrf9myYA/s64/photo.jpg","userId":"06393280560991383795"}},"colab":{"base_uri":"https://localhost:8080/","height":493}},"cell_type":"code","source":["train('simple_tag', 'Predator-Prey')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Using good policy maddpg and adv policy maddpg\n","Starting iterations...\n","steps: 24975, episodes: 1000, mean episode reward: -0.7477632799887421, time: 115.819\n","steps: 49975, episodes: 2000, mean episode reward: 0.8306453678608287, time: 152.266\n","steps: 74975, episodes: 3000, mean episode reward: 8.071925852293045, time: 154.644\n","steps: 99975, episodes: 4000, mean episode reward: 11.215985832614242, time: 154.179\n","steps: 124975, episodes: 5000, mean episode reward: 15.816438693348195, time: 155.074\n","steps: 149975, episodes: 6000, mean episode reward: 31.043099904831497, time: 154.976\n","steps: 174975, episodes: 7000, mean episode reward: 45.941660431644095, time: 155.23\n","steps: 199975, episodes: 8000, mean episode reward: 15.255968870078606, time: 153.372\n","steps: 224975, episodes: 9000, mean episode reward: 9.737429120350772, time: 156.003\n","steps: 249975, episodes: 10000, mean episode reward: 11.372254000965379, time: 154.017\n","steps: 274975, episodes: 11000, mean episode reward: 11.863740383728349, time: 153.5\n","steps: 299975, episodes: 12000, mean episode reward: 12.562737895128976, time: 151.644\n","steps: 324975, episodes: 13000, mean episode reward: 14.353086698081428, time: 156.616\n","steps: 349975, episodes: 14000, mean episode reward: 12.307309423374866, time: 155.777\n","steps: 374975, episodes: 15000, mean episode reward: 12.355524059998311, time: 156.428\n","steps: 399975, episodes: 16000, mean episode reward: 14.58274902912046, time: 154.386\n","steps: 424975, episodes: 17000, mean episode reward: 14.004756691849176, time: 155.423\n","steps: 449975, episodes: 18000, mean episode reward: 16.81510803391823, time: 154.077\n","steps: 474975, episodes: 19000, mean episode reward: 22.840108618127616, time: 154.733\n","steps: 499975, episodes: 20000, mean episode reward: 23.7075070150765, time: 153.324\n","steps: 524975, episodes: 21000, mean episode reward: 27.05475141471875, time: 157.09\n","steps: 549975, episodes: 22000, mean episode reward: 29.54250417781759, time: 155.061\n","steps: 574975, episodes: 23000, mean episode reward: 26.54095226477522, time: 154.642\n","steps: 599975, episodes: 24000, mean episode reward: 20.93631117591999, time: 151.532\n","steps: 624975, episodes: 25000, mean episode reward: 15.181398744119154, time: 155.706\n","...Finished total of 25001 episodes.\n"],"name":"stdout"}]},{"metadata":{"id":"fmhciDUpZZz9","colab_type":"text"},"cell_type":"markdown","source":["### 7. Cooperative Navigation\n","Agents are rewarded based on how far any agent is from each landmark. Agents are penalized if they collide with other agents. So, agents have to learn to cover all the landmarks while avoiding collisions."]},{"metadata":{"id":"KqsuBpbgZ0mf","colab_type":"code","outputId":"33e73b58-c551-4de3-9aaa-30b9f69b5775","executionInfo":{"status":"ok","timestamp":1543143727416,"user_tz":-480,"elapsed":2845084,"user":{"displayName":"Saul Leung","photoUrl":"https://lh4.googleusercontent.com/--atDq9mMZEc/AAAAAAAAAAI/AAAAAAAAcVs/twfLrf9myYA/s64/photo.jpg","userId":"06393280560991383795"}},"colab":{"base_uri":"https://localhost:8080/","height":493}},"cell_type":"code","source":["train('simple_spread', 'Cooperative Navigation')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Using good policy maddpg and adv policy maddpg\n","Starting iterations...\n","steps: 24975, episodes: 1000, mean episode reward: -665.2543330896325, time: 90.716\n","steps: 49975, episodes: 2000, mean episode reward: -826.7683860285059, time: 113.599\n","steps: 74975, episodes: 3000, mean episode reward: -604.6051438575689, time: 114.696\n","steps: 99975, episodes: 4000, mean episode reward: -575.0436901392027, time: 114.855\n","steps: 124975, episodes: 5000, mean episode reward: -553.4622219828843, time: 113.471\n","steps: 149975, episodes: 6000, mean episode reward: -539.4998262682473, time: 115.385\n","steps: 174975, episodes: 7000, mean episode reward: -533.3811456742297, time: 112.966\n","steps: 199975, episodes: 8000, mean episode reward: -524.2825599889181, time: 112.772\n","steps: 224975, episodes: 9000, mean episode reward: -521.1189904402252, time: 113.504\n","steps: 249975, episodes: 10000, mean episode reward: -515.2612397304559, time: 113.224\n","steps: 274975, episodes: 11000, mean episode reward: -508.6429990212346, time: 114.421\n","steps: 299975, episodes: 12000, mean episode reward: -508.23635212859267, time: 115.623\n","steps: 324975, episodes: 13000, mean episode reward: -503.64958092623425, time: 115.066\n","steps: 349975, episodes: 14000, mean episode reward: -497.10762713925135, time: 115.785\n","steps: 374975, episodes: 15000, mean episode reward: -492.1481055675816, time: 114.091\n","steps: 399975, episodes: 16000, mean episode reward: -488.6256993851062, time: 114.352\n","steps: 424975, episodes: 17000, mean episode reward: -485.101551304273, time: 113.697\n","steps: 449975, episodes: 18000, mean episode reward: -483.9253676075427, time: 112.465\n","steps: 474975, episodes: 19000, mean episode reward: -481.8656057324755, time: 114.685\n","steps: 499975, episodes: 20000, mean episode reward: -479.5232893641691, time: 115.124\n","steps: 524975, episodes: 21000, mean episode reward: -484.16152030089756, time: 115.049\n","steps: 549975, episodes: 22000, mean episode reward: -478.578445072542, time: 115.84\n","steps: 574975, episodes: 23000, mean episode reward: -475.67777494311775, time: 115.679\n","steps: 599975, episodes: 24000, mean episode reward: -476.63863383616297, time: 114.485\n","steps: 624975, episodes: 25000, mean episode reward: -475.15572844390414, time: 114.145\n","...Finished total of 25001 episodes.\n"],"name":"stdout"}]},{"metadata":{"id":"t8oYOa3lZSzU","colab_type":"text"},"cell_type":"markdown","source":["### 8. Physical Deception\n","All agents observe position of landmarks and other agents. One landmark is the ‘target landmark’ (colored green). Good agents rewarded based on how close one of them is to the target landmark, but negatively rewarded if the adversary is close to target landmark. Adversary is rewarded based on how close it is to the target, but it doesn’t know which landmark is the target landmark. So good agents have to learn to ‘split up’ and cover all landmarks to deceive the adversary."]},{"metadata":{"id":"_C1akMFAZvqk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":493},"outputId":"95e23f3a-57cb-44a0-feed-33d249ee11d2","executionInfo":{"status":"ok","timestamp":1543147377530,"user_tz":-480,"elapsed":2602300,"user":{"displayName":"Saul Leung","photoUrl":"https://lh4.googleusercontent.com/--atDq9mMZEc/AAAAAAAAAAI/AAAAAAAAcVs/twfLrf9myYA/s64/photo.jpg","userId":"06393280560991383795"}}},"cell_type":"code","source":["train('simple_adversary', 'Physical Deception')"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Using good policy maddpg and adv policy maddpg\n","Starting iterations...\n","steps: 24975, episodes: 1000, mean episode reward: -23.62287264995385, time: 80.808\n","steps: 49975, episodes: 2000, mean episode reward: -24.570445068769427, time: 104.695\n","steps: 74975, episodes: 3000, mean episode reward: 4.230458852692893, time: 103.531\n","steps: 99975, episodes: 4000, mean episode reward: 3.057586729468592, time: 104.73\n","steps: 124975, episodes: 5000, mean episode reward: 2.491181899654724, time: 105.074\n","steps: 149975, episodes: 6000, mean episode reward: 2.2235228714947364, time: 105.138\n","steps: 174975, episodes: 7000, mean episode reward: 1.2179255089826488, time: 104.638\n","steps: 199975, episodes: 8000, mean episode reward: -0.22520570266500975, time: 105.392\n","steps: 224975, episodes: 9000, mean episode reward: -0.6101349851035407, time: 103.833\n","steps: 249975, episodes: 10000, mean episode reward: -0.12715706710949184, time: 105.014\n","steps: 274975, episodes: 11000, mean episode reward: 0.10080468771879497, time: 105.693\n","steps: 299975, episodes: 12000, mean episode reward: 0.11624995206551808, time: 103.957\n","steps: 324975, episodes: 13000, mean episode reward: -0.3845608117217645, time: 104.634\n","steps: 349975, episodes: 14000, mean episode reward: 0.1617969236519678, time: 104.677\n","steps: 374975, episodes: 15000, mean episode reward: 0.2852842215002403, time: 104.391\n","steps: 399975, episodes: 16000, mean episode reward: 0.4745545682187476, time: 104.733\n","steps: 424975, episodes: 17000, mean episode reward: 0.3107245402691144, time: 105.304\n","steps: 449975, episodes: 18000, mean episode reward: -0.005099236186461639, time: 103.785\n","steps: 474975, episodes: 19000, mean episode reward: -0.08931504632205337, time: 104.916\n","steps: 499975, episodes: 20000, mean episode reward: -0.2199806746433017, time: 104.638\n","steps: 524975, episodes: 21000, mean episode reward: 0.1266974931798863, time: 104.716\n","steps: 549975, episodes: 22000, mean episode reward: 0.2797338469462449, time: 104.802\n","steps: 574975, episodes: 23000, mean episode reward: 0.9982479096048412, time: 105.379\n","steps: 599975, episodes: 24000, mean episode reward: 0.7468303745774602, time: 104.765\n","steps: 624975, episodes: 25000, mean episode reward: 0.658032274738867, time: 104.055\n","...Finished total of 25001 episodes.\n"],"name":"stdout"}]},{"metadata":{"id":"MWlMro0ymJ23","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}